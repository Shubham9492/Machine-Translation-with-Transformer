{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.736829Z","iopub.execute_input":"2024-10-19T05:03:28.737285Z","iopub.status.idle":"2024-10-19T05:03:28.747220Z","shell.execute_reply.started":"2024-10-19T05:03:28.737243Z","shell.execute_reply":"2024-10-19T05:03:28.745336Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport math\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.750445Z","iopub.execute_input":"2024-10-19T05:03:28.751064Z","iopub.status.idle":"2024-10-19T05:03:28.763641Z","shell.execute_reply.started":"2024-10-19T05:03:28.751001Z","shell.execute_reply":"2024-10-19T05:03:28.761784Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1] \n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n    print(f\"scaled.size() : {scaled.size()}\")\n    if mask is not None:\n        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n        scaled += mask # 30 x 8 x 200 x 200\n    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n    values = torch.matmul(attention, v) # 30 x 8 x 200 x 64\n    return values, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.765470Z","iopub.execute_input":"2024-10-19T05:03:28.766089Z","iopub.status.idle":"2024-10-19T05:03:28.780999Z","shell.execute_reply.started":"2024-10-19T05:03:28.766033Z","shell.execute_reply":"2024-10-19T05:03:28.779215Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        #  x: 30 x 200 x 512\n        x = self.linear1(x) #30 x 200 x 2048\n        x = self.relu(x) #30 x 200 x 2048\n        x = self.dropout(x) #30 x 200 x 2048\n        print(f\"x after dropout layer: {x.size()}\")\n        x = self.linear2(x) #30 x 200 x 512\n        return x #30 x 200 x 512\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.783270Z","iopub.execute_input":"2024-10-19T05:03:28.784230Z","iopub.status.idle":"2024-10-19T05:03:28.803279Z","shell.execute_reply.started":"2024-10-19T05:03:28.784180Z","shell.execute_reply":"2024-10-19T05:03:28.801404Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # 512\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # 512\n\n    def forward(self, inputs):\n        # inputs : 30 x 200 x 512\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))] # [-1]\n        print(f\"dims: {dims}\")\n        mean = inputs.mean(dim=dims, keepdim=True) #30 x 200 x 1\n        print(f\"Mean ({mean.size()})\")\n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # 30 x 200 x 512\n        std = (var + self.eps).sqrt() # 30 x 200 x 512\n        y = (inputs - mean) / std # 30 x 200 x 512\n        out = self.gamma * y  + self.beta  # 30 x 200 x 512\n        print(f\"out: {out.size()}\")\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.807154Z","iopub.execute_input":"2024-10-19T05:03:28.807787Z","iopub.status.idle":"2024-10-19T05:03:28.820754Z","shell.execute_reply.started":"2024-10-19T05:03:28.807723Z","shell.execute_reply":"2024-10-19T05:03:28.819127Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model , 3 * d_model) # 1536 \n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask=None):\n        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512 \n        print(f\"x.size(): {x.size()}\")\n        qkv = self.qkv_layer(x) # 30 x 200 x 1536\n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n        qkv = qkv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 192\n        q, k, v = qkv.chunk(3, dim=-1) # q: 30 x 8 x 200 x 64, k: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n        values, attention = scaled_dot_product(q, k, v, mask) # values: 30 x 8 x 200 x 64\n        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim) # 30 x 200 x 512\n        out = self.linear_layer(values) # 30 x 200 x 512\n        return out # 30 x 200 x 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.822512Z","iopub.execute_input":"2024-10-19T05:03:28.822985Z","iopub.status.idle":"2024-10-19T05:03:28.852960Z","shell.execute_reply.started":"2024-10-19T05:03:28.822943Z","shell.execute_reply":"2024-10-19T05:03:28.850347Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class MultiHeadCrossAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model , 2 * d_model) # 1024\n        self.q_layer = nn.Linear(d_model , d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, y, mask=None):\n        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512\n        kv = self.kv_layer(x) # 30 x 200 x 1024\n        q = self.q_layer(y) # 30 x 200 x 512\n        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)  # 30 x 200 x 8 x 128\n        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)  # 30 x 200 x 8 x 64\n        kv = kv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 128\n        q = q.permute(0, 2, 1, 3) # 30 x 8 x 200 x 64\n        k, v = kv.chunk(2, dim=-1) # K: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n        values, attention = scaled_dot_product(q, k, v, mask) #  30 x 8 x 200 x 64\n        values = values.reshape(batch_size, sequence_length, d_model) #  30 x 200 x 512\n        out = self.linear_layer(values)  #  30 x 200 x 512\n        return out  #  30 x 200 x 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.854727Z","iopub.execute_input":"2024-10-19T05:03:28.855254Z","iopub.status.idle":"2024-10-19T05:03:28.872382Z","shell.execute_reply.started":"2024-10-19T05:03:28.855200Z","shell.execute_reply":"2024-10-19T05:03:28.870652Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y, decoder_mask):\n        _y = y # 30 x 200 x 512\n        print(\"MASKED SELF ATTENTION\")\n        y = self.self_attention(y, mask=decoder_mask) # 30 x 200 x 512\n        y = self.dropout1(y) # 30 x 200 x 512\n        y = self.norm1(y + _y) # 30 x 200 x 512\n\n        _y = y # 30 x 200 x 512\n        print(\"CROSS ATTENTION\")\n        y = self.encoder_decoder_attention(x, y, mask=None) #30 x 200 x 512\n        y = self.dropout2(y)\n        y = self.norm2(y + _y)  #30 x 200 x 512\n\n        _y = y  #30 x 200 x 512\n        print(\"FEED FORWARD 1\")\n        y = self.ffn(y) #30 x 200 x 512\n        y = self.dropout3(y) #30 x 200 x 512\n        y = self.norm3(y + _y) #30 x 200 x 512\n        return y #30 x 200 x 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.876215Z","iopub.execute_input":"2024-10-19T05:03:28.876722Z","iopub.status.idle":"2024-10-19T05:03:28.899547Z","shell.execute_reply.started":"2024-10-19T05:03:28.876673Z","shell.execute_reply":"2024-10-19T05:03:28.897893Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, mask) #30 x 200 x 512\n        return y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.901509Z","iopub.execute_input":"2024-10-19T05:03:28.903447Z","iopub.status.idle":"2024-10-19T05:03:28.919598Z","shell.execute_reply.started":"2024-10-19T05:03:28.903359Z","shell.execute_reply":"2024-10-19T05:03:28.917927Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n        super().__init__()\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n                                          for _ in range(num_layers)])\n\n    def forward(self, x, y, mask):\n        #x : 30 x 200 x 512 \n        #y : 30 x 200 x 512\n        #mask : 200 x 200\n        y = self.layers(x, y, mask)\n        return y #30 x 200 x 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.921383Z","iopub.execute_input":"2024-10-19T05:03:28.921891Z","iopub.status.idle":"2024-10-19T05:03:28.933067Z","shell.execute_reply.started":"2024-10-19T05:03:28.921838Z","shell.execute_reply":"2024-10-19T05:03:28.931488Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"d_model = 512\nnum_heads = 8\ndrop_prob = 0.1\nbatch_size = 30\nmax_sequence_length = 200\nffn_hidden = 2048\nnum_layers = 5\n\nx = torch.randn( (batch_size, max_sequence_length, d_model) ) \ny = torch.randn( (batch_size, max_sequence_length, d_model) )  \nmask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\nmask = torch.triu(mask, diagonal=1)\ndecoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\nout = decoder(x, y, mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:28.934952Z","iopub.execute_input":"2024-10-19T05:03:28.935444Z","iopub.status.idle":"2024-10-19T05:03:35.482083Z","shell.execute_reply.started":"2024-10-19T05:03:28.935397Z","shell.execute_reply":"2024-10-19T05:03:35.479721Z"}},"outputs":[{"name":"stdout","text":"MASKED SELF ATTENTION\nx.size(): torch.Size([30, 200, 512])\nscaled.size() : torch.Size([30, 8, 200, 200])\n-- ADDING MASK of shape torch.Size([200, 200]) --\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nCROSS ATTENTION\nscaled.size() : torch.Size([30, 8, 200, 200])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nFEED FORWARD 1\nx after dropout layer: torch.Size([30, 200, 2048])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nMASKED SELF ATTENTION\nx.size(): torch.Size([30, 200, 512])\nscaled.size() : torch.Size([30, 8, 200, 200])\n-- ADDING MASK of shape torch.Size([200, 200]) --\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nCROSS ATTENTION\nscaled.size() : torch.Size([30, 8, 200, 200])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nFEED FORWARD 1\nx after dropout layer: torch.Size([30, 200, 2048])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nMASKED SELF ATTENTION\nx.size(): torch.Size([30, 200, 512])\nscaled.size() : torch.Size([30, 8, 200, 200])\n-- ADDING MASK of shape torch.Size([200, 200]) --\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nCROSS ATTENTION\nscaled.size() : torch.Size([30, 8, 200, 200])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nFEED FORWARD 1\nx after dropout layer: torch.Size([30, 200, 2048])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nMASKED SELF ATTENTION\nx.size(): torch.Size([30, 200, 512])\nscaled.size() : torch.Size([30, 8, 200, 200])\n-- ADDING MASK of shape torch.Size([200, 200]) --\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nCROSS ATTENTION\nscaled.size() : torch.Size([30, 8, 200, 200])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nFEED FORWARD 1\nx after dropout layer: torch.Size([30, 200, 2048])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nMASKED SELF ATTENTION\nx.size(): torch.Size([30, 200, 512])\nscaled.size() : torch.Size([30, 8, 200, 200])\n-- ADDING MASK of shape torch.Size([200, 200]) --\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nCROSS ATTENTION\nscaled.size() : torch.Size([30, 8, 200, 200])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\nFEED FORWARD 1\nx after dropout layer: torch.Size([30, 200, 2048])\ndims: [-1]\nMean (torch.Size([30, 200, 1]))\nout: torch.Size([30, 200, 512])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T05:03:42.311023Z","iopub.execute_input":"2024-10-19T05:03:42.311520Z","iopub.status.idle":"2024-10-19T05:03:42.342299Z","shell.execute_reply.started":"2024-10-19T05:03:42.311477Z","shell.execute_reply":"2024-10-19T05:03:42.340752Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n        [0., 0., -inf,  ..., -inf, -inf, -inf],\n        [0., 0., 0.,  ..., -inf, -inf, -inf],\n        ...,\n        [0., 0., 0.,  ..., 0., -inf, -inf],\n        [0., 0., 0.,  ..., 0., 0., -inf],\n        [0., 0., 0.,  ..., 0., 0., 0.]])"},"metadata":{}}],"execution_count":24}]}