{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2119948,"sourceType":"datasetVersion","datasetId":1272055}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:25.326152Z","iopub.status.idle":"2024-10-19T07:06:25.326541Z","shell.execute_reply.started":"2024-10-19T07:06:25.326335Z","shell.execute_reply":"2024-10-19T07:06:25.326353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nimport torch.nn.functional as F\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:25.457178Z","iopub.execute_input":"2024-10-19T07:06:25.457529Z","iopub.status.idle":"2024-10-19T07:06:30.163857Z","shell.execute_reply.started":"2024-10-19T07:06:25.457494Z","shell.execute_reply":"2024-10-19T07:06:30.162945Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# device = torch.device('cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:30.165699Z","iopub.execute_input":"2024-10-19T07:06:30.166135Z","iopub.status.idle":"2024-10-19T07:06:30.203886Z","shell.execute_reply.started":"2024-10-19T07:06:30.166101Z","shell.execute_reply":"2024-10-19T07:06:30.202560Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"English_file = \"/kaggle/input/samanantar/final_data/en-hi/train.en\"\nHindi_file = \"/kaggle/input/samanantar/final_data/en-hi/train.hi\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:30.205915Z","iopub.execute_input":"2024-10-19T07:06:30.206607Z","iopub.status.idle":"2024-10-19T07:06:30.229847Z","shell.execute_reply.started":"2024-10-19T07:06:30.206560Z","shell.execute_reply":"2024-10-19T07:06:30.228869Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"START_TOKEN = '<START>'\nPADDING_TOKEN = '<PADDING>'\nEND_TOKEN = '<END>'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:30.231975Z","iopub.execute_input":"2024-10-19T07:06:30.232265Z","iopub.status.idle":"2024-10-19T07:06:30.239925Z","shell.execute_reply.started":"2024-10-19T07:06:30.232234Z","shell.execute_reply":"2024-10-19T07:06:30.239051Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Hindi letters and symbols\nhindi_letters = [\n    'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न',\n    'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', 'ँ', '्', 'ऽ', 'ॐ',\n    '॒', '॑', '़', '॓', '॔'\n]\n\n# Hindi numerals\nhindi_numerals = [chr(code) for code in range(0x0966, 0x096F + 1)]\nprint(hindi_numerals)\n\n# Common symbols and punctuation used in Hindi text\nother_symbols = [\n    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n    ':', ';', '<', '=', '>', '?'\n]\n\n\nHindi_vocab = [START_TOKEN] + hindi_letters + hindi_numerals + other_symbols + [PADDING_TOKEN, END_TOKEN]\n\n# Example usage\nprint(\"Hindi Vocabulary:\")\nprint(Hindi_vocab)\nprint(len(Hindi_vocab))\nprint(len(set(Hindi_vocab)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:30.241096Z","iopub.execute_input":"2024-10-19T07:06:30.241403Z","iopub.status.idle":"2024-10-19T07:06:30.255999Z","shell.execute_reply.started":"2024-10-19T07:06:30.241372Z","shell.execute_reply":"2024-10-19T07:06:30.255084Z"}},"outputs":[{"name":"stdout","text":"['०', '१', '२', '३', '४', '५', '६', '७', '८', '९']\nHindi Vocabulary:\n['<START>', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', 'ँ', '्', 'ऽ', 'ॐ', '॒', '॑', '़', '॓', '॔', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '<PADDING>', '<END>']\n114\n114\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n\n\nEnglish_vocab = [\n    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n    ':', '<', '=', '>', '?', '@',\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n    'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n    'Y', 'Z',\n    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n    'y', 'z',\n    PADDING_TOKEN, END_TOKEN\n] \n \n# Print the length of English vocabulary\nprint(\"English vocabulary:\", (English_vocab))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:30.257099Z","iopub.execute_input":"2024-10-19T07:06:30.257453Z","iopub.status.idle":"2024-10-19T07:06:30.277048Z","shell.execute_reply.started":"2024-10-19T07:06:30.257408Z","shell.execute_reply":"2024-10-19T07:06:30.276072Z"}},"outputs":[{"name":"stdout","text":"English vocabulary: ['<START>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<PADDING>', '<END>']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"index_to_hindi = {k:v for k,v in enumerate(Hindi_vocab)}\nhindi_to_index = {v:k for k,v in enumerate(Hindi_vocab)}\nindex_to_english = {k:v for k,v in enumerate(English_vocab)}\nenglish_to_index = {v:k for k,v in enumerate(English_vocab)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:30.278192Z","iopub.execute_input":"2024-10-19T07:06:30.278479Z","iopub.status.idle":"2024-10-19T07:06:30.284078Z","shell.execute_reply.started":"2024-10-19T07:06:30.278448Z","shell.execute_reply":"2024-10-19T07:06:30.283109Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"with open(Hindi_file, 'r') as file:\n    hindi_sentences = file.readlines()\nwith open(English_file, 'r' ) as file:\n    english_sentences = file.readlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:06:35.997152Z","iopub.execute_input":"2024-10-19T07:06:35.997985Z","iopub.status.idle":"2024-10-19T07:07:11.119022Z","shell.execute_reply.started":"2024-10-19T07:06:35.997945Z","shell.execute_reply":"2024-10-19T07:07:11.118007Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(len(hindi_sentences))\nprint(len(english_sentences))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:11.120730Z","iopub.execute_input":"2024-10-19T07:07:11.121067Z","iopub.status.idle":"2024-10-19T07:07:11.128405Z","shell.execute_reply.started":"2024-10-19T07:07:11.121034Z","shell.execute_reply":"2024-10-19T07:07:11.127402Z"}},"outputs":[{"name":"stdout","text":"8568307\n8568307\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for i,j  in zip(hindi_sentences[110:120],english_sentences[110:120]):\n    print(j,i)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:11.129525Z","iopub.execute_input":"2024-10-19T07:07:11.130210Z","iopub.status.idle":"2024-10-19T07:07:11.142156Z","shell.execute_reply.started":"2024-10-19T07:07:11.130166Z","shell.execute_reply":"2024-10-19T07:07:11.141073Z"}},"outputs":[{"name":"stdout","text":"The Vice President of India, Shri M Venkaiah Naidu today urged the youth to take a firm resolve to shun negativism and join the forces of growth.\n उपराष्ट्रपति, श्री एम वेंकैया नायडू ने छात्रों से नकारात्मकता छोड़कर रचनात्‍मक दृष्टि विकसित करने का आग्रह किया है।\n\nHe is at present lodged in a prison in London.\n ऐसे में उसे अभी लंदन की जेल में दिन गुजरने होंगे.\n\nConsidering that crop segment constitutes a dominant component of the GVA computation, its performance is very critical.\n यह ध्यान में रखते हुए कि फसल खंड जीवीए कंप्यूटेशन का एक प्रमुख घटक हैं, इसका प्रदर्शन बहुत महत्वपूर्ण है।\n\nIn both conditions your doctor can prescribe you some analgesics or other medicines, or he can refer you to another specialist or surgeon.\n इन दोनों अवस्थाओं में आराम पहुंचाने के लिये आपका डाक्टर आप को दर्द नाशक अन्य दवाईयों का नुस्खा लिख कर दे सकता है या सर्जन या अन्य विशेषग्य से आप की सहायता करने को कह सकता है.\n\nHowever, Prasad, who is also the Information Technology Minister, said it would not be wise to restrict the social media just because of a challenge, as the social media has empowered the common man by increasing his access to information and power to ask questions.\n सूचना प्रौद्योगिकी मंत्री प्रसाद ने हालांकि कहा कि सिर्फ एक चुनौती पेश करने के कारण सोशल मीडिया को प्रतिबंधित करना बुद्धिमानी नहीं होगी क्योंकि सोशल मीडिया आम आदमी की जानकारी बढ़ाकर और उसकी प्रश्न करने की क्षमता बढ़ाकर उसे सशक्त भी कर रहा है।\n\nData integrity and security are pressing issues for electronic commerce.\n डेटा अखंडता और सुरक्षा इलेक्ट्रॉनिक कॉमर्स के लिए बहुत गर्म और अहम मुद्दे हैं।\n\nHe repeated this assertion several times.\n ’’ यही उक्ति उन्होंने आगे चलकर कई बार दोहराई।\n\nAnd they swear by Allah that they are from among you while they are not from among you. but they are a people who are afraid.\n और (मुसलमानों) ये लोग ख़ुदा की क़सम खाएंगे फिर वह तुममें ही के हैं हालॉकि वह लोग तुममें के नहीं हैं मगर हैं ये लोग बुज़दिल हैं\n\nWith everyones cooperation we will soon defeat Corona completely in the state, he said.\n ’’ सोरेन ने कहा, ‘‘सभी के सहयोग से हम कोरोना को हरायेंगे।\n\nShe shared pictures on her Instagram stories.\n उन्होंने अपने दोस्तों के साथ की फोटोज इंस्टाग्राम स्टोरी में शेयर की।\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"total_sentences = 600000\nenglish_sntcs = english_sentences[:total_sentences]\nhindi_sntcs = hindi_sentences[:total_sentences]\nenglish_sntcs = [sentence.rstrip('\\n') for sentence in english_sntcs]\nhindi_sntcs = [sentence.rstrip('\\n') for sentence in hindi_sntcs]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:11.144568Z","iopub.execute_input":"2024-10-19T07:07:11.144924Z","iopub.status.idle":"2024-10-19T07:07:11.551324Z","shell.execute_reply.started":"2024-10-19T07:07:11.144883Z","shell.execute_reply":"2024-10-19T07:07:11.550528Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"%%time\nmax_seq_len = 200\n\ndef is_valid_tokens(sentence, vocab):\n    for token in list(set(sentence)):\n        if token not in vocab:\n            return False\n    return True\n\ndef is_valid_length(sentence, max_sequence_length):\n    return len(list(sentence)) < (max_sequence_length - 1) # need to add <START> and <END> tokens\n\nvalid_sntcs_idx = []\nfor index in range(len(hindi_sntcs)):\n    hin_sntc , eng_sntc = hindi_sntcs[index], english_sntcs[index]\n    if is_valid_length(hin_sntc, max_seq_len) \\\n        and is_valid_length(eng_sntc, max_seq_len) \\\n        and is_valid_tokens(hin_sntc, Hindi_vocab)\\\n        and is_valid_tokens(eng_sntc, English_vocab):\n        \n        valid_sntcs_idx.append(index)\n        \nprint(len(valid_sntcs_idx))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:11.552430Z","iopub.execute_input":"2024-10-19T07:07:11.552728Z","iopub.status.idle":"2024-10-19T07:07:37.331487Z","shell.execute_reply.started":"2024-10-19T07:07:11.552696Z","shell.execute_reply":"2024-10-19T07:07:37.330616Z"}},"outputs":[{"name":"stdout","text":"154603\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"english_sntcs = [english_sntcs[i] for i in valid_sntcs_idx]\nhindi_sntcs = [hindi_sntcs[i] for i in valid_sntcs_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.332526Z","iopub.execute_input":"2024-10-19T07:07:37.332807Z","iopub.status.idle":"2024-10-19T07:07:37.410902Z","shell.execute_reply.started":"2024-10-19T07:07:37.332763Z","shell.execute_reply":"2024-10-19T07:07:37.410108Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass TextDataset(Dataset):\n    \n    def __init__(self, english_sntcs, hindi_sntcs):\n        self.english_sntcs = english_sntcs\n        self.hindi_sntcs = hindi_sntcs\n        \n    def __len__(self):\n        return len(self.english_sntcs)\n    \n    def __getitem__(self,idx):\n        return self.english_sntcs[idx], self.hindi_sntcs[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.411977Z","iopub.execute_input":"2024-10-19T07:07:37.412265Z","iopub.status.idle":"2024-10-19T07:07:37.417950Z","shell.execute_reply.started":"2024-10-19T07:07:37.412233Z","shell.execute_reply":"2024-10-19T07:07:37.417096Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"dataset = TextDataset(english_sntcs, hindi_sntcs)\nlen(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.419131Z","iopub.execute_input":"2024-10-19T07:07:37.419407Z","iopub.status.idle":"2024-10-19T07:07:37.428222Z","shell.execute_reply.started":"2024-10-19T07:07:37.419377Z","shell.execute_reply":"2024-10-19T07:07:37.427366Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"154603"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"batch_size = 32\n\ntrain_loader = DataLoader(dataset, batch_size)\nlen(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.429395Z","iopub.execute_input":"2024-10-19T07:07:37.429949Z","iopub.status.idle":"2024-10-19T07:07:37.438066Z","shell.execute_reply.started":"2024-10-19T07:07:37.429916Z","shell.execute_reply":"2024-10-19T07:07:37.437239Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"4832"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import torch\n\nd_model = 512\nffn_hidden = 2048\nnum_heads = 8\ndrop_prob = 0.1\nnum_layers = 2\nmax_sequence_length = max_seq_len = 200\nhin_vocab_size = len(Hindi_vocab)\n\ntransformer = Transformer(d_model, \n                          ffn_hidden,\n                          num_heads, \n                          drop_prob, \n                          num_layers, \n                          max_seq_len,\n                          hin_vocab_size,\n                          english_to_index,\n                          hindi_to_index,\n                          START_TOKEN, \n                          END_TOKEN, \n                          PADDING_TOKEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:08:32.128866Z","iopub.execute_input":"2024-10-19T07:08:32.129501Z","iopub.status.idle":"2024-10-19T07:08:32.319165Z","shell.execute_reply.started":"2024-10-19T07:08:32.129463Z","shell.execute_reply":"2024-10-19T07:08:32.318152Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torch import nn\n\ncriterian = nn.CrossEntropyLoss(ignore_index=hindi_to_index[PADDING_TOKEN],\n                                reduction='none')\n\n# When computing the loss, we are ignoring cases when the label is the padding token\nfor params in transformer.parameters():\n    if params.dim() > 1:\n        nn.init.xavier_uniform_(params)\n\noptim = torch.optim.Adam(transformer.parameters(), lr=0.0001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:08:35.736245Z","iopub.execute_input":"2024-10-19T07:08:35.737044Z","iopub.status.idle":"2024-10-19T07:08:37.860273Z","shell.execute_reply.started":"2024-10-19T07:08:35.737004Z","shell.execute_reply":"2024-10-19T07:08:37.859448Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"NEG_INFTY = -1e9\n\ndef create_masks(eng_batch, kn_batch):\n    num_sentences = len(eng_batch)\n    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n\n    for idx in range(num_sentences):\n        eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n        kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n        decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n        decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n        decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n\n    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:08:38.170553Z","iopub.execute_input":"2024-10-19T07:08:38.171363Z","iopub.status.idle":"2024-10-19T07:08:38.180836Z","shell.execute_reply.started":"2024-10-19T07:08:38.171322Z","shell.execute_reply":"2024-10-19T07:08:38.179887Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nNEG_INFTY = -1e9\n\ndef create_masks1(eng_batch, hin_batch):\n    num_sentences = len(eng_batch)\n    look_ahead_mask = torch.full([max_seq_len, max_seq_len] , True)\n    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n    encoder_padding_mask = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n    decoder_padding_mask_self_attention = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n    \n    for idx in range(num_sentences):\n        eng_sentence_length, hin_sentence_length = len(eng_batch[idx]), len(hin_batch[idx])\n#         print(eng_sentence_length,hin_sentence_length)\n        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_seq_len)\n        hin_chars_to_padding_mask = np.arange(hin_sentence_length + 1, max_seq_len)\n        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n        decoder_padding_mask_self_attention[idx, :, hin_chars_to_padding_mask] = True\n        decoder_padding_mask_self_attention[idx, hin_chars_to_padding_mask, :] = True\n        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n        decoder_padding_mask_cross_attention[idx, hin_chars_to_padding_mask, :] = True\n\n    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :50, :50]}\")\n    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :50, :50]}\")\n    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :50, :50]}\")\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n        \n# Corrected function call with provided sentences\nhin_sentence = (\"आप कैसे हैं\",)\neng_sentence = (\"should we go to the mall?\",)\nencoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks1(eng_sentence, hin_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:08:38.591465Z","iopub.execute_input":"2024-10-19T07:08:38.592118Z","iopub.status.idle":"2024-10-19T07:08:38.734266Z","shell.execute_reply.started":"2024-10-19T07:08:38.592079Z","shell.execute_reply":"2024-10-19T07:08:38.733305Z"}},"outputs":[{"name":"stdout","text":"encoder_self_attention_mask torch.Size([1, 200, 200]): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        ...,\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09]])\ndecoder_self_attention_mask torch.Size([1, 200, 200]): tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        ...,\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09]])\ndecoder_cross_attention_mask torch.Size([1, 200, 200]): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        ...,\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09]])\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"%%time\ntransformer.train()\nprint(device)\ntransformer.to(device)\ntotal_loss = 0\nnum_epochs = 10\nprint(train_loader)\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch}\")\n    iterator = iter(train_loader)\n    print(len(train_loader))\n    for batch_num, batch in enumerate(iterator):\n        transformer.train()\n        eng_batch, hin_batch = batch\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, hin_batch)\n#         print(encoder_self_attention_mask.shape,decoder_self_attention_mask.shape, decoder_cross_attention_mask.shape)\n#         print(encoder_self_attention_mask[0][0],encoder_self_attention_mask[1][0],encoder_self_attention_mask[0][2])\n        optim.zero_grad()\n        hin_predictions = transformer(eng_batch,\n                                     hin_batch,\n                                     encoder_self_attention_mask.to(device), \n                                     decoder_self_attention_mask.to(device), \n                                     decoder_cross_attention_mask.to(device),\n                                     enc_start_token=False,\n                                     enc_end_token=False,\n                                     dec_start_token=True,\n                                     dec_end_token=True)\n        labels = transformer.decoder.sentence_embedding.batch_tokenize(hin_batch, start_token=False, end_token=True)\n        loss = criterian(\n            hin_predictions.view(-1, hin_vocab_size).to(device),\n            labels.view(-1).to(device)\n        ).to(device)\n        valid_indicies = torch.where(labels.view(-1) == hindi_to_index[PADDING_TOKEN], False, True)\n        loss = loss.sum() / valid_indicies.sum()\n        loss.backward()\n        optim.step()\n        #train_losses.append(loss.item())\n        if batch_num % 1000 == 0:\n            print(f\"Iteration {batch_num} : {loss.item()}\")\n            print(f\"English: {eng_batch[0]}\")\n            print(f\"Hindi Translation: {hin_batch[0]}\")\n#             print(\"Mask\",encoder_self_attention_mask[0,:50,:50])\n            hin_sentence_predicted = torch.argmax(hin_predictions[0], axis=1)\n            predicted_sentence = \"\"\n            for idx in hin_sentence_predicted:\n                if idx == hindi_to_index[END_TOKEN]:\n                    break\n                predicted_sentence += index_to_hindi[idx.item()]\n            print(f\"Hindi Prediction: {predicted_sentence}\")\n\n\n            transformer.eval()\n            hin_sentence = (\"\",)\n            eng_sentence = (\"should we go to the mall?\",)\n#             print(encoder_self_attention_mask.shape,decoder_self_attention_mask.shape, decoder_cross_attention_mask.shape)\n#             print(encoder_self_attention_mask[0][0],encoder_self_attention_mask[0][1],encoder_self_attention_mask[0][2])\n               \n            for word_counter in range(max_seq_len):\n                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, hin_sentence)\n                predictions = transformer(eng_sentence,\n                                          hin_sentence,\n                                          encoder_self_attention_mask.to(device), \n                                          decoder_self_attention_mask.to(device), \n                                          decoder_cross_attention_mask.to(device),\n                                          enc_start_token=False,\n                                          enc_end_token=False,\n                                          dec_start_token=True,\n                                          dec_end_token=False)\n                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n                next_token_index = torch.argmax(next_token_prob_distribution).item()\n                next_token = index_to_hindi[next_token_index]\n                hin_sentence = (hin_sentence[0] + next_token, )\n                if next_token == END_TOKEN:\n                    break\n            \n            print(f\"Evaluation translation (should we go to the mall?) : {hin_sentence}\")\n            print(\"-------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:15:49.321706Z","iopub.execute_input":"2024-10-19T07:15:49.322099Z","iopub.status.idle":"2024-10-19T09:07:05.747726Z","shell.execute_reply.started":"2024-10-19T07:15:49.322061Z","shell.execute_reply":"2024-10-19T09:07:05.746745Z"}},"outputs":[{"name":"stdout","text":"cuda\n<torch.utils.data.dataloader.DataLoader object at 0x7b90292c2bf0>\nEpoch 0\n4832\nIteration 0 : 4.214349269866943\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: कंककाकाककककूकककरकककााकाककककककककरकककककााकककहंाूंहंकहररारााकाहंहहहहहाह्हहहकानहहह कह करर ककरंाकारहरर्ाकंाहककर ंेक  कह ह्क रकरहकककि हहकाहककह काहहहहककहहहरहकाहहाहाााााहरारांहांाहहाकरकहाहााााकाूााााकहककककरका\nEvaluation translation (should we go to the mall?) : ('ाााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााा',)\n-------------------------------------------\nIteration 1000 : 2.6333467960357666\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: इनेहों   केा के का ा ् कारीक  ़कीर ंके केरा   यहे ा कै.\nEvaluation translation (should we go to the mall?) : ('इस में के का के का की है.<END>',)\n-------------------------------------------\nIteration 2000 : 2.3239283561706543\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: कोनें केवीं  कया  केाऱ किहिम     1  2    1  0  2    2         1      1 1 1112121    22  1  122    2  1     2  11       2   42 1   11 1          11     11  1 1 21  1  1   1 12\"1   1 \nEvaluation translation (should we go to the mall?) : ('उन्होंने कहा है.<END>',)\n-------------------------------------------\nIteration 3000 : 2.237598180770874\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: प्र् क्ल्  काल्काने      1                                    2           1     2  2  4          4 14    2    1       2     1 2 21 1            1 1221     11  12   \"   1  \nEvaluation translation (should we go to the mall?) : ('क्या क्या है कि सकते हैं?<END>',)\n-------------------------------------------\nIteration 4000 : 2.050266742706299\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: इ र  सस किा के कसी  के सह्ी   काद  कै हा्े ाहैंऔर सनके सारारकास्ेंकिग ककि्या केल्    करल्   के े है  औकेाा सेा काह   हैं.कर सका हा  हाि    ककैंकर  लार रहै हहक े  हाद ाय हे काा हान यालााहै\nEvaluation translation (should we go to the mall?) : ('क्या क्या क्या है कि क्या है?<END>',)\n-------------------------------------------\nEpoch 1\n4832\nIteration 0 : 1.937737226486206\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: इिसका माान कें क्सिसे बन्छा मारुपई हर हे.\nEvaluation translation (should we go to the mall?) : ('क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 1000 : 1.8568893671035767\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: मन्होंने कहा कि पाचेपी काननककसीकायेंको माज्ी ि कोने है.\nEvaluation translation (should we go to the mall?) : ('क्या क्या क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.7184698581695557\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: इे ों केरींा हयत  हे द  किकहलि                                                              म                 \nEvaluation translation (should we go to the mall?) : ('क्या क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 3000 : 1.8322677612304688\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: स्र् क्र्श काज्कार\nEvaluation translation (should we go to the mall?) : ('क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 4000 : 1.7669628858566284\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: इ र  तस बहह के बसी  कह जह़ीन  कादा कै कत्ा  कै कर वनकी बारादकास्ांकोगोकसह्त  हेख्    हरला   कर े हैं औके े हास हढत   कैं\nEvaluation translation (should we go to the mall?) : ('क्या है कि क्या है कि क्या है?<END>',)\n-------------------------------------------\nEpoch 2\n4832\nIteration 0 : 1.6958094835281372\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: इिसके बााब कें सरसिसे कप्छी कारू त हर ही.\nEvaluation translation (should we go to the mall?) : ('क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 1000 : 1.6175644397735596\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: \"न्होंने कहा कि काजेपी कार ककडीकायिंके काजनीतिककीता है.\nEvaluation translation (should we go to the mall?) : ('क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.5464797019958496\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: सि ों केकांा किरी मेसजे किकुमी  \nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 3000 : 1.686684489250183\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: स्र् सरक ज काज्कारे\nEvaluation translation (should we go to the mall?) : ('क्या आपके पास करते हैं?<END>',)\n-------------------------------------------\nIteration 4000 : 1.648642897605896\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: यऐर  मस बहह के हबी  कहसवह़ीन  क़दा कै का्ार कै कर कनके बांा कास्ा कोगोककहहत  हेझ् त  करल्र  कर े हैं औऔे े बास तात र है .और उकह की  कह़ीन  कखै औि  ह़दा कै मकम े  काराेह केाद ा कनद्थ रा है\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि क्या है?<END>',)\n-------------------------------------------\nEpoch 3\n4832\nIteration 0 : 1.5994783639907837\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: इिसके बााब कें सररिसहतकप्छी कारू त हर गी.\nEvaluation translation (should we go to the mall?) : ('क्या हम क्या करना चाहिए?<END>',)\n-------------------------------------------\nIteration 1000 : 1.522383689880371\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: \"न्होंने कहा कि काजेपी काररककडीकायापके साज्ीति केता है.\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.4829362630844116\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: जिनों केरांा कारा हेसरे किक्ंीर \nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 3000 : 1.6206247806549072\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: प्र् क्मेज कुजरकांा   \nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 4000 : 1.5692784786224365\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: यऐर  तस पहह के कअी) कह वह़ीन  क़दा की हााा  कै और जनके बावा कास्ा कगग कमह्त) केस् त  क्लार  किते हैं औऔबस) के  हात   है\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि वह क्या है?<END>',)\n-------------------------------------------\nEpoch 4\n4832\nIteration 0 : 1.5104737281799316\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: पिसके ब्ान कें काकिपेतकप्छे कारूआत की जी.\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या करना चाहिए?<END>',)\n-------------------------------------------\nIteration 1000 : 1.4669358730316162\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: उन्होंने कहा,कि कीजेपी काररककट कायांके साजनीतिकके े है.\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.4005573987960815\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: कि ों केराया कयरा हे र  किकामीर \nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि वे क्या है?<END>',)\n-------------------------------------------\nIteration 3000 : 1.5318619012832642\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: स्री क्केज पंणाकातत   क\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या हो सकते हैं?<END>',)\n-------------------------------------------\nIteration 4000 : 1.5103936195373535\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: यऐर  अस दहह के कखी) कह यह़ीनन क़दा की मााा  कै और उसके पावायकोन्े कोगोहमह्त))हेस्बत  करलार  कीते हैं औबेसे पा  कडत   कैं और जजा)ही) का़ीन  कसै)औि  ह़द  कै हबब)े) केर्ेद केाद ादकनलाबाद  है\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि वह क्या है?<END>',)\n-------------------------------------------\nEpoch 5\n4832\nIteration 0 : 1.4495022296905518\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: इिसके बराब कें पाकिपे कप्छा तारूआत कर जी.\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या करना चाहिए?<END>',)\n-------------------------------------------\nIteration 1000 : 1.4235994815826416\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: \"न्होंने कहा कि वीजेपी कापरसकड सारिपके साज्ीतिकके े है.\nEvaluation translation (should we go to the mall?) : ('क्या आप को क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.3371995687484741\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: जि ों केराया कााी हे जी किकामीर       क  क क     कश क क   क कक  त            क क  क                   ककक   क क                                ड    क   क    प क          क   कक   क            \nEvaluation translation (should we go to the mall?) : ('क्या आप क्या है कि वे क्या है?<END>',)\n-------------------------------------------\nIteration 3000 : 1.509232521057129\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: पार् कुक्ज कोणरकातक  कक  पकककककक प क त  ककक \nEvaluation translation (should we go to the mall?) : ('क्या आप क्या होगा?<END>',)\n-------------------------------------------\nIteration 4000 : 1.4628971815109253\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: इऐर  तस तकह से ककी) के जह़ीन  क़दा की कासम  कै और तसके पावादकिन्ी उोगो(मह्त))हेतलबत  करलार  करते हैं औऔब ो)बा  कडत य कैं और उमे ली) हा़ीनन कऔै)औि  ह़दा की कऔा े) कडलाेध केादाानकनलाब़ल़ाकै\nEvaluation translation (should we go to the mall?) : ('क्या आप क्या कर रहे हैं?<END>',)\n-------------------------------------------\nEpoch 6\n4832\nIteration 0 : 1.422627329826355\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: पिसका बााब में पाकिपेतकप्छी कारूआत ही जी.\nEvaluation translation (should we go to the mall?) : ('क्या आप को क्या करना चाहिए?<END>',)\n-------------------------------------------\nIteration 1000 : 1.397820234298706\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: बन्होंने कहा कि कीजेपी काजरसकट कायासको साजनीति के ा है.\nEvaluation translation (should we go to the mall?) : ('क्या आप को क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.3177199363708496\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: का ों केराला (ारा (े ज  हिकामी  ककप  पप क क   कक कककक  त      कप     त    पपश  ल       कन     प प   प  कक  क  कपप न क           तशकत क            ल क क   क पकन   प प  प   त क  प   प  \nEvaluation translation (should we go to the mall?) : ('क्या आप इस बात करेंगे?<END>',)\n-------------------------------------------\nIteration 3000 : 1.4740651845932007\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: संर् करकेज कोणरकाते पपपप कपपकपपकपकपकककपकप ककपपपकपशपकपकवककपकतपशपपकसपपपकपपकन पपपपपकपपपकपपक कपपपपपकपपकककपपतकककककककपकपकपपपपप पकशपन पपकपक पकपकपपल पपककप  पकलपकतपपकशकककपपपपपपपकपप  पकपक प\nEvaluation translation (should we go to the mall?) : ('क्या आपको क्या है कि क्या है?<END>',)\n-------------------------------------------\nIteration 4000 : 1.4126827716827393\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: यऐर) तस कहह)से कखी) केसते़ीन  क़दा की कासार कै और उसकी पावा किस्े (ोगोककह़त))हेसलबत) करकारा कर े हैं औकब ))बा  कडत र कैं और उउह ली) कह़ीनन कमै)औि  ह़दा की कहख े) कडरा्द)केीजीा हाराऱऱरकै\nEvaluation translation (should we go to the mall?) : ('क्या तुम उन्हें क्या होगा?<END>',)\n-------------------------------------------\nEpoch 7\n4832\nIteration 0 : 1.362099289894104\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: इासके बराब में पाकिसे कप्छी सारूआत हर सी.\nEvaluation translation (should we go to the mall?) : ('क्या आप को क्या है?<END>',)\n-------------------------------------------\nIteration 1000 : 1.3538097143173218\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: \"न्होंने कहा कि भीजेपी कानरककड केरासको साजनीतिकके े है.\nEvaluation translation (should we go to the mall?) : ('क्या आप को किसी को समय करते हैं?<END>',)\n-------------------------------------------\nIteration 2000 : 1.2646269798278809\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: कि ों केरांत (यरा (ेकरा किकामीर  कपककककतककक  क क  \nEvaluation translation (should we go to the mall?) : ('क्या आप उनके लिए क्या है?<END>',)\n-------------------------------------------\nIteration 3000 : 1.438954472541809\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: पार् प्केज काज पाने पककपपपककपपककपकककककपनककप\nEvaluation translation (should we go to the mall?) : ('क्या आप को कहां सकते हैं?<END>',)\n-------------------------------------------\nIteration 4000 : 1.3846098184585571\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: ययर  तस तहह हे)ककी) केतते़ीनन क़दा की कासा ाखै और उस े पावा कोस्े तोगोहकह़त))केसलह ) करकारा करते हैं तऔुल))का  काह ल है  और उखह ली) कह़ीनन हखै)औि  तुदा कै ककब े) कनलााध के यीानकनराद़द़ कै\nEvaluation translation (should we go to the mall?) : ('क्या हम क्या हम करने के लिए क्या है?<END>',)\n-------------------------------------------\nEpoch 8\n4832\nIteration 0 : 1.3197413682937622\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: पिसके पराब में पाकिपे पप्छी सारूआत की सी.\nEvaluation translation (should we go to the mall?) : ('क्या हम क्या होगा?<END>',)\n-------------------------------------------\nIteration 1000 : 1.3287380933761597\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: बन्होंने कहा कि भीजेपी कारीककड कारोडको बाजनीतिककोना है.\nEvaluation translation (should we go to the mall?) : ('क्या आप को किसी को समय करेंगे?<END>',)\n-------------------------------------------\nIteration 2000 : 1.2432383298873901\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: डाहों केरायत (यरा (े जि काकामी\nEvaluation translation (should we go to the mall?) : ('क्या आप इस बात करेंगे?<END>',)\n-------------------------------------------\nIteration 3000 : 1.3976151943206787\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: पां् काकेज कोपाकलना कपककप ककपकपपपककपश कककपकपकपकककककपशककककककककपकपकककककककककपपकलपप कपककक ककपककककककप पवकककपकककक शकक कपककपकक  क    क क शपक क ककक\nEvaluation translation (should we go to the mall?) : ('क्या हम कभी को समय करते हैं?<END>',)\n-------------------------------------------\nIteration 4000 : 1.362793207168579\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: औऐर  यस तहह से ककी) कह)उे़ीन  क़दा की केका़ कै और जसके पावा किस े खोगो(खह्त )रेखलबत) करलार  हरते हैं औऔा)))बा  हात ल कै  और खखा खी) हे़ीनन ककै)औि  ज़दा कै कजब े) बखलााध केदद ा हुलाद़रा कै\nEvaluation translation (should we go to the mall?) : ('क्या हम उसके बारे में क्या होगा?<END>',)\n-------------------------------------------\nEpoch 9\n4832\nIteration 0 : 1.3096197843551636\nEnglish: In reply, Pakistan got off to a solid start.\nHindi Translation: जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\nHindi Prediction: पिसके बवाब में पाकिकह कप्छी तारूआत कर जी.\nEvaluation translation (should we go to the mall?) : ('क्या हम उन्हें को बताया है?<END>',)\n-------------------------------------------\nIteration 1000 : 1.29529869556427\nEnglish: \"\"\"The BJP is only doing politics over cow,\"\" he claimed.\"\nHindi Translation: उन्होंने कहा कि बीजेपी यूज एंड थ्रो की राजनीति करती है.\nHindi Prediction: उन्होंने कहा कि भीजेपी कान पकड काराडको साजनीति काना है.\nEvaluation translation (should we go to the mall?) : ('क्या हम क्या हम क्या है?<END>',)\n-------------------------------------------\nIteration 2000 : 1.1800166368484497\nEnglish: Dark green (in colour).\nHindi Translation: दोनों निहायत गहरे सब्ज़ व शादाब\nHindi Prediction: डाबों केरायत (ंरा (े जीाकाकामी  कशकशककत ककककंकतशककतंकक कलवततत त तकतततकककनक  क  कककककककककककक कलल ग  कतककककक ककशन  कलक क शनलककक  शककतककतततश लककनकककपपनकल लतककतककपततककककततकककगकत ककककककककक क तककककक ककक नत \nEvaluation translation (should we go to the mall?) : ('क्या हम उनके लिए क्या होगा?<END>',)\n-------------------------------------------\nIteration 3000 : 1.3749043941497803\nEnglish: Loss of Complete Package\nHindi Translation: पूरा पैकेट गुम जाना\nHindi Prediction: पारी काकेट कोल्कतरे    श  ककककपककककककशशककशककक कपशश   व क  क कतकककक क कप पकप ककक    क क क क  क   शवक कककककककककककककक       क कक क क क  क  क  क  कश शकशनककक प कककककन कककककककककककककककक  कप कवक क   पक  क  क \nEvaluation translation (should we go to the mall?) : ('क्या हम उनके लिए क्या होगा?<END>',)\n-------------------------------------------\nIteration 4000 : 1.3174337148666382\nEnglish: That is because Allah is the Truth, and that which they call upon other than Him is falsehood, and because Allah is the Most High, the Grand.\nHindi Translation: (और) इस वजह से (भी) कि यक़ीनन खुदा ही बरहक़ है और उसके सिवा जिनको लोग (वक्ते मुसीबत) पुकारा करते हैं (सबके सब) बातिल हैं और (ये भी) यक़ीनी (है कि) खुदा ही (सबसे) बुलन्द मर्तबा बुर्जुग़ है\nHindi Prediction: औऐर  तस दजहोसे ककी) क़ उे़ीनन क़दा की कााक़ कै कर खसको पावा कोस्ो औोगोकखह्त))हेदीबत) करकारा कर े हैं औकब े)कब  कात ल है\nEvaluation translation (should we go to the mall?) : ('क्या हम उन्हें क्या होगा?<END>',)\n-------------------------------------------\nCPU times: user 3h 38min 17s, sys: 4min 10s, total: 3h 42min 28s\nWall time: 1h 51min 16s\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"print(hindi_to_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:07:19.542168Z","iopub.execute_input":"2024-10-19T09:07:19.543043Z","iopub.status.idle":"2024-10-19T09:07:19.547562Z","shell.execute_reply.started":"2024-10-19T09:07:19.543002Z","shell.execute_reply":"2024-10-19T09:07:19.546641Z"}},"outputs":[{"name":"stdout","text":"{'<START>': 0, 'अ': 1, 'आ': 2, 'इ': 3, 'ई': 4, 'उ': 5, 'ऊ': 6, 'ऋ': 7, 'ऌ': 8, 'ए': 9, 'ऐ': 10, 'ओ': 11, 'औ': 12, 'क': 13, 'ख': 14, 'ग': 15, 'घ': 16, 'ङ': 17, 'च': 18, 'छ': 19, 'ज': 20, 'झ': 21, 'ञ': 22, 'ट': 23, 'ठ': 24, 'ड': 25, 'ढ': 26, 'ण': 27, 'त': 28, 'थ': 29, 'द': 30, 'ध': 31, 'न': 32, 'प': 33, 'फ': 34, 'ब': 35, 'भ': 36, 'म': 37, 'य': 38, 'र': 39, 'ल': 40, 'ळ': 41, 'व': 42, 'श': 43, 'ष': 44, 'स': 45, 'ह': 46, 'ा': 47, 'ि': 48, 'ी': 49, 'ु': 50, 'ू': 51, 'ृ': 52, 'ॄ': 53, 'ॅ': 54, 'े': 55, 'ै': 56, 'ो': 57, 'ौ': 58, 'ं': 59, 'ः': 60, 'ँ': 61, '्': 62, 'ऽ': 63, 'ॐ': 64, '॒': 65, '॑': 66, '़': 67, '॓': 68, '॔': 69, '०': 70, '१': 71, '२': 72, '३': 73, '४': 74, '५': 75, '६': 76, '७': 77, '८': 78, '९': 79, ' ': 80, '!': 81, '\"': 82, '#': 83, '$': 84, '%': 85, '&': 86, \"'\": 87, '(': 88, ')': 89, '*': 90, '+': 91, ',': 92, '-': 93, '.': 94, '/': 95, '0': 96, '1': 97, '2': 98, '3': 99, '4': 100, '5': 101, '6': 102, '7': 103, '8': 104, '9': 105, ':': 106, ';': 107, '<': 108, '=': 109, '>': 110, '?': 111, '<PADDING>': 112, '<END>': 113}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"transformer.eval()\ndef translate(eng_sentence):\n    eng_sentence = (eng_sentence,)\n    kn_sentence = (\"\",)\n    for word_counter in range(max_sequence_length):\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n        predictions = transformer(eng_sentence,\n                                  kn_sentence,\n                                  encoder_self_attention_mask.to(device), \n                                  decoder_self_attention_mask.to(device), \n                                  decoder_cross_attention_mask.to(device),\n                                  enc_start_token=False,\n                                  enc_end_token=False,\n                                  dec_start_token=True,\n                                  dec_end_token=False)\n        next_token_prob_distribution = predictions[0][word_counter]\n        next_token_index = torch.argmax(next_token_prob_distribution).item()\n        next_token = index_to_hindi[next_token_index]\n        kn_sentence = (kn_sentence[0] + next_token, )\n        if next_token == END_TOKEN:\n            break\n    return kn_sentence[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:07:24.818913Z","iopub.execute_input":"2024-10-19T09:07:24.819548Z","iopub.status.idle":"2024-10-19T09:07:24.827569Z","shell.execute_reply.started":"2024-10-19T09:07:24.819509Z","shell.execute_reply":"2024-10-19T09:07:24.826328Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"translation = translate(\"Don't do that\")\nprint(translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:07:29.943545Z","iopub.execute_input":"2024-10-19T09:07:29.943946Z","iopub.status.idle":"2024-10-19T09:07:30.086136Z","shell.execute_reply.started":"2024-10-19T09:07:29.943909Z","shell.execute_reply":"2024-10-19T09:07:30.084982Z"}},"outputs":[{"name":"stdout","text":"न करें न करें<END>\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"translation = translate(\"where are you?\")\nprint(translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:07:33.625281Z","iopub.execute_input":"2024-10-19T09:07:33.626235Z","iopub.status.idle":"2024-10-19T09:07:33.792797Z","shell.execute_reply.started":"2024-10-19T09:07:33.626181Z","shell.execute_reply":"2024-10-19T09:07:33.791962Z"}},"outputs":[{"name":"stdout","text":"कहां कहां है?<END>\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"translation = translate(\"who are you?\")\nprint(translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:07:38.558078Z","iopub.execute_input":"2024-10-19T09:07:38.558692Z","iopub.status.idle":"2024-10-19T09:07:38.781926Z","shell.execute_reply.started":"2024-10-19T09:07:38.558652Z","shell.execute_reply":"2024-10-19T09:07:38.780997Z"}},"outputs":[{"name":"stdout","text":"किसे किसके पास किसके ?<END>\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"translation = translate(\"I am fine\")\nprint(translation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:11:27.187860Z","iopub.execute_input":"2024-10-19T09:11:27.188609Z","iopub.status.idle":"2024-10-19T09:11:27.365664Z","shell.execute_reply.started":"2024-10-19T09:11:27.188570Z","shell.execute_reply":"2024-10-19T09:11:27.364701Z"}},"outputs":[{"name":"stdout","text":"मैं मैं कर रहा हूँ<END>\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import sentencepiece as spm\n\n# Example Hindi text data\ntrain_data = \"\"\"\nमैं मशीन लर्निंग सीख रहा हूं|\nयह मेरा दूसरा उदाहरण है|\n\"\"\"\n\n# Save the training data to a file\nwith open('hindi_text.txt', 'w', encoding='utf-8') as f:\n    f.write(train_data)\n\n# Train SentencePiece model with a target vocabulary size\nvocab_size = 50  # You can adjust this size as needed\n\nspm.SentencePieceTrainer.train(input='hindi_text.txt', model_prefix='hindi', vocab_size=vocab_size, character_coverage=1.0, model_type='bpe')\n\n# Load the trained model\nsp = spm.SentencePieceProcessor()\nsp.load('hindi.model')\n\n# Test the tokenizer\ntest_sentence = \"यह सिर्फ एक उदाहरण है|\"\ntokens = sp.encode_as_pieces(test_sentence)\ntoken_ids = sp.encode_as_ids(test_sentence)\n\nprint(\"Tokens:\", tokens)\nprint(\"Token IDs:\", token_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:12:58.184234Z","iopub.execute_input":"2024-10-19T09:12:58.184589Z","iopub.status.idle":"2024-10-19T09:12:58.197556Z","shell.execute_reply.started":"2024-10-19T09:12:58.184558Z","shell.execute_reply":"2024-10-19T09:12:58.196432Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['▁यह', '▁', 'स', 'ि', 'र्', 'फ', '▁', 'एक', '▁उद', 'ाह', 'रण', '▁ह', 'ै', '|']\nToken IDs: [26, 27, 36, 47, 13, 0, 27, 0, 25, 16, 11, 5, 39, 33]\n","output_type":"stream"},{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: hindi_text.txt\n  input_format: \n  model_prefix: hindi\n  model_type: BPE\n  vocab_size: 50\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: hindi_text.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 2 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=55\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=23\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 2 sentences.\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2\ntrainer_interface.cc(609) LOG(INFO) Done! 11\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=20 all=74 active=51 piece=शीन\ntrainer_interface.cc(687) LOG(INFO) Saving model: hindi.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: hindi.vocab\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n    if mask is not None:\n        scaled = scaled.permute(1, 0, 2, 3) + mask\n        scaled = scaled.permute(1, 0, 2, 3)\n    attention = F.softmax(scaled, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.441400Z","iopub.execute_input":"2024-10-19T07:07:37.441869Z","iopub.status.idle":"2024-10-19T07:07:37.447703Z","shell.execute_reply.started":"2024-10-19T07:07:37.441837Z","shell.execute_reply":"2024-10-19T07:07:37.446878Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_sequence_length):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.d_model = d_model\n\n    def forward(self):\n        even_i = torch.arange(0, self.d_model, 2).float()\n        denominator = torch.pow(10000, even_i/self.d_model)\n        position = (torch.arange(self.max_sequence_length)\n                          .reshape(self.max_sequence_length, 1))\n        even_PE = torch.sin(position / denominator)\n        odd_PE = torch.cos(position / denominator)\n        stacked = torch.stack([even_PE, odd_PE], dim=2)\n        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n        return PE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.448788Z","iopub.execute_input":"2024-10-19T07:07:37.449070Z","iopub.status.idle":"2024-10-19T07:07:37.465012Z","shell.execute_reply.started":"2024-10-19T07:07:37.449040Z","shell.execute_reply":"2024-10-19T07:07:37.463999Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class SentenceEmbedding(nn.Module):\n    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n        super().__init__()\n        self.vocab_size = len(language_to_index)\n        self.max_sequence_length = max_sequence_length\n        self.embedding = nn.Embedding(self.vocab_size, d_model)\n        self.language_to_index = language_to_index\n        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n        self.dropout = nn.Dropout(p=0.1)\n        self.START_TOKEN = START_TOKEN\n        self.END_TOKEN = END_TOKEN\n        self.PADDING_TOKEN = PADDING_TOKEN\n    \n    def batch_tokenize(self, batch, start_token, end_token):\n        def tokenize(sentence, start_token, end_token):\n            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n            if start_token:\n                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n            if end_token:\n                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n            return torch.tensor(sentence_word_indicies)\n\n        tokenized = []\n        for sentence_num in range(len(batch)):\n            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n        tokenized = torch.stack(tokenized)\n        return tokenized.to(device)\n    \n    def forward(self, x, start_token, end_token): # sentence\n        x = self.batch_tokenize(x, start_token, end_token)\n        x = self.embedding(x)\n        pos = self.position_encoder().to(device)\n        x = self.dropout(x + pos)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.465978Z","iopub.execute_input":"2024-10-19T07:07:37.466242Z","iopub.status.idle":"2024-10-19T07:07:37.477162Z","shell.execute_reply.started":"2024-10-19T07:07:37.466198Z","shell.execute_reply":"2024-10-19T07:07:37.476285Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask):\n        batch_size, sequence_length, d_model = x.size()\n        qkv = self.qkv_layer(x)\n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n        out = self.linear_layer(values)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.478251Z","iopub.execute_input":"2024-10-19T07:07:37.478542Z","iopub.status.idle":"2024-10-19T07:07:37.491632Z","shell.execute_reply.started":"2024-10-19T07:07:37.478509Z","shell.execute_reply":"2024-10-19T07:07:37.490812Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n\n    def forward(self, inputs):\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n        mean = inputs.mean(dim=dims, keepdim=True)\n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n        std = (var + self.eps).sqrt()\n        y = (inputs - mean) / std\n        out = self.gamma * y + self.beta\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.492734Z","iopub.execute_input":"2024-10-19T07:07:37.493028Z","iopub.status.idle":"2024-10-19T07:07:37.506231Z","shell.execute_reply.started":"2024-10-19T07:07:37.492998Z","shell.execute_reply":"2024-10-19T07:07:37.505373Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.507241Z","iopub.execute_input":"2024-10-19T07:07:37.507531Z","iopub.status.idle":"2024-10-19T07:07:37.515381Z","shell.execute_reply.started":"2024-10-19T07:07:37.507491Z","shell.execute_reply":"2024-10-19T07:07:37.514571Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, self_attention_mask):\n        residual_x = x.clone()\n        x = self.attention(x, mask=self_attention_mask)\n        x = self.dropout1(x)\n        x = self.norm1(x + residual_x)\n        residual_x = x.clone()\n        x = self.ffn(x)\n        x = self.dropout2(x)\n        x = self.norm2(x + residual_x)\n        return x # 30 x 200 x 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.516530Z","iopub.execute_input":"2024-10-19T07:07:37.516824Z","iopub.status.idle":"2024-10-19T07:07:37.525359Z","shell.execute_reply.started":"2024-10-19T07:07:37.516789Z","shell.execute_reply":"2024-10-19T07:07:37.524561Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class SequentialEncoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, self_attention_mask  = inputs\n        for module in self._modules.values():\n            x = module(x, self_attention_mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.526388Z","iopub.execute_input":"2024-10-19T07:07:37.526691Z","iopub.status.idle":"2024-10-19T07:07:37.538141Z","shell.execute_reply.started":"2024-10-19T07:07:37.526657Z","shell.execute_reply":"2024-10-19T07:07:37.537435Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, \n                 d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob, \n                 num_layers,\n                 max_sequence_length,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN, \n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n                                      for _ in range(num_layers)])\n\n    def forward(self, x, self_attention_mask, start_token, end_token):\n        x = self.sentence_embedding(x, start_token, end_token)\n        x = self.layers(x, self_attention_mask)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.539101Z","iopub.execute_input":"2024-10-19T07:07:37.539452Z","iopub.status.idle":"2024-10-19T07:07:37.548854Z","shell.execute_reply.started":"2024-10-19T07:07:37.539405Z","shell.execute_reply":"2024-10-19T07:07:37.548146Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class MultiHeadCrossAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n        self.q_layer = nn.Linear(d_model , d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, y, mask):\n        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n        kv = self.kv_layer(x)\n        q = self.q_layer(y)\n        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n        kv = kv.permute(0, 2, 1, 3)\n        q = q.permute(0, 2, 1, 3)\n        k, v = kv.chunk(2, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n        out = self.linear_layer(values)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.549905Z","iopub.execute_input":"2024-10-19T07:07:37.550174Z","iopub.status.idle":"2024-10-19T07:07:37.563721Z","shell.execute_reply.started":"2024-10-19T07:07:37.550142Z","shell.execute_reply":"2024-10-19T07:07:37.562962Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n        _y = y.clone()\n        y = self.self_attention(y, mask=self_attention_mask)\n        y = self.dropout1(y)\n        y = self.layer_norm1(y + _y)\n\n        _y = y.clone()\n        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n        y = self.dropout2(y)\n        y = self.layer_norm2(y + _y)\n\n        _y = y.clone()\n        y = self.ffn(y)\n        y = self.dropout3(y)\n        y = self.layer_norm3(y + _y)\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.565015Z","iopub.execute_input":"2024-10-19T07:07:37.565360Z","iopub.status.idle":"2024-10-19T07:07:37.576590Z","shell.execute_reply.started":"2024-10-19T07:07:37.565325Z","shell.execute_reply":"2024-10-19T07:07:37.575885Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, self_attention_mask, cross_attention_mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, self_attention_mask, cross_attention_mask)\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.578316Z","iopub.execute_input":"2024-10-19T07:07:37.578605Z","iopub.status.idle":"2024-10-19T07:07:37.590650Z","shell.execute_reply.started":"2024-10-19T07:07:37.578574Z","shell.execute_reply":"2024-10-19T07:07:37.589983Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, \n                 d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob, \n                 num_layers,\n                 max_seq_len,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN, \n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_seq_len, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n        y = self.sentence_embedding(y, start_token, end_token)\n        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.591799Z","iopub.execute_input":"2024-10-19T07:07:37.592137Z","iopub.status.idle":"2024-10-19T07:07:37.600571Z","shell.execute_reply.started":"2024-10-19T07:07:37.592100Z","shell.execute_reply":"2024-10-19T07:07:37.599756Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, \n                d_model, \n                ffn_hidden, \n                num_heads, \n                drop_prob, \n                num_layers,\n                max_seq_len, \n                hin_vocab_size,\n                english_to_index,\n                hindi_to_index,\n                START_TOKEN, \n                END_TOKEN, \n                PADDING_TOKEN\n                ):\n        super().__init__()\n        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_seq_len, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_seq_len, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.linear = nn.Linear(d_model, hin_vocab_size)\n\n    def forward(self, \n                x, \n                y, \n                encoder_self_attention_mask=None, \n                decoder_self_attention_mask=None, \n                decoder_cross_attention_mask=None,\n                enc_start_token=False,\n                enc_end_token=False,\n                dec_start_token=False, # We should make this true\n                dec_end_token=False): # x, y are batch of sentences\n        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n        out = self.linear(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T07:07:37.601624Z","iopub.execute_input":"2024-10-19T07:07:37.602238Z","iopub.status.idle":"2024-10-19T07:07:37.612908Z","shell.execute_reply.started":"2024-10-19T07:07:37.602206Z","shell.execute_reply":"2024-10-19T07:07:37.612122Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}